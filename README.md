# MULTI_PROMPT_TEXT2KG

This repository contains the official implementation of our research framework for **Ontology-Aware Knowledge Graph Construction from Unstructured Text using Large Language Models (LLMs)**.

The system combines multiple prompting strategies with a hierarchical evaluator to improve extraction accuracy while mitigating hallucinations. It is designed and evaluated on the **Text2KGBench** benchmark.

---

# ğŸš€ Overview

Knowledge Graph (KG) construction from natural language is challenging due to:

- Incomplete supervision  
- Ontology constraints  
- Hallucinated entities and relations  
- Inconsistent triple formatting  

To address these issues, we propose a **multi-prompt ensemble framework** consisting of:

- ğŸ” **Structured Multi-Step Reasoning (ToT-based)**
- ğŸ“š **Ontology-Constrained OpenIE Prompt**
- âš¡ **General Ontology-Aware Extraction Prompt**
- ğŸ§  **Hierarchical Evaluator (Rules Aâ€“C)**

The evaluator filters candidate triples using cross-prompt agreement, explicit evidence scoring, and textual similarity measures to reduce hallucinations and enforce schema compliance.

---

# ğŸ“‚ Data Directory

## ğŸŸ¢ Input Data

### ğŸ“‚ `data/input/input_text/`

Raw input sentences for:

- **DBpediaâ€“WebNLG**
- **Wikidataâ€“TekGen**

Subdirectories:
- `dbpedia/`
- `wikidata/`

---

### ğŸ“‚ `data/input/ground_truth/`

Gold standard SPO triples used to compute:

- Precision
- Recall
- F1-score

Subdirectories:
- `dbpedia/`
- `wikidata/`

---

### ğŸ“‚ `data/input/fewshots_example/`

Few-shot examples injected into prompts to guide ontology-aligned extraction.

Subdirectories:
- `dbpedia/`
- `wikidata/`

---

### ğŸ“‚ `data/input/ontology/old_ontology/`

Domain-specific ontology schemas including:

- Concept definitions  
- Relation signatures  
- Domainâ€“range constraints  

Subdirectories:
- `dbpedia/`
- `wikidata/`

---

### ğŸ“‚ `data/train_data/`

Training datasets used for **LLaMA-3 fine-tuning**.

- `dbpedia/`
- `wikidata/`

Wikidata includes a synthetic enrichment pipeline:

- `synthetic_train_data/wikidata_input_train/`
- `synthetic_train_data/wikidata_output_train/`

The filtered output is directly used for supervised fine-tuning.

---

# ğŸ“¤ Output Data

## ğŸ” Multi-Prompt Extraction Outputs

Located in:

`data/output/multi_step_prompts/`

- `TOT_dfs/` â†’ Tree-of-Thoughts structured extraction  
- `Open_IE_prompt/` â†’ Ontology-constrained OpenIE  
- `general_extraction_prompt/` â†’ Lightweight SPO extraction  

---

## ğŸ§  Evaluator-Filtered Outputs

`data/output/evaluator_filtered_output/`

Final merged triple sets after applying:

- Rule A â€“ Cross-Prompt Consensus  
- Rule B â€“ Evidence-Based Validation  
- Rule C â€“ Similarity-Based Filtering  

---

## ğŸ“Š Evaluation Results

`data/output/metrics_evaluation/`

Contains dataset-wise evaluation:

- `dbpedia/`
- `wikidata/`

Metrics reported:

- Precision (P)
- Recall (R)
- F1-score (F1)
- Ontology Conformance (OC â†‘)
- Subject Hallucination (SH â†“)
- Relation Hallucination (RH â†“)
- Object Hallucination (OH â†“)

---

# ğŸ§  Source Code (`src/`)

Main implementation directory.

Contains:

- Synthetic data generation  
- Model fine-tuning  
- Multi-prompt extraction  
- Evaluator logic  
- Evaluation metrics  

---

## ğŸ”¹ Data Preparation & Training

### `Synthetic_train_data_generation_7B.py`

Generates ontology-filtered synthetic triples for Wikidataâ€“TekGen.

---

### `Llama_finetuned.py`

Supervised fine-tuning (SFT) of LLaMA-3-8B-Instruct using LoRA/QLoRA.

---

## ğŸ” Multi-Prompt Extraction Modules

Located in:

`src/multi_prompt_extractor/`

- `Open_IE_prompt.py`
- `general_extraction_prompt.py`

---

### `evaluator.py`

Implements hierarchical triple verification:

- Cross-Prompt Consensus
- Evidence-Based Validation
- Similarity-Based Filtering

---

# ğŸ“ Project Directory Structure

```text
MULTI_PROMPT_TEXT2KG/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/
â”‚   â”‚   â”œâ”€â”€ input_text/
â”‚   â”‚   â”‚   â”œâ”€â”€ dbpedia/
â”‚   â”‚   â”‚   â””â”€â”€ wikidata/
â”‚   â”‚   â”œâ”€â”€ ground_truth/
â”‚   â”‚   â”‚   â”œâ”€â”€ dbpedia/
â”‚   â”‚   â”‚   â””â”€â”€ wikidata/
â”‚   â”‚   â”œâ”€â”€ fewshots_example/
â”‚   â”‚   â”‚   â”œâ”€â”€ dbpedia/
â”‚   â”‚   â”‚   â””â”€â”€ wikidata/
â”‚   â”‚   â””â”€â”€ ontology/
â”‚   â”‚       â””â”€â”€ old_ontology/
â”‚   â”‚           â”œâ”€â”€ dbpedia/
â”‚   â”‚           â””â”€â”€ wikidata/
â”‚   â”‚
â”‚   â”œâ”€â”€ train_data/
â”‚   â”‚   â”œâ”€â”€ dbpedia/
â”‚   â”‚   â””â”€â”€ wikidata/
â”‚   â”‚       â””â”€â”€ synthetic_train_data/
â”‚   â”‚           â”œâ”€â”€ wikidata_input_train/
â”‚   â”‚           â””â”€â”€ wikidata_output_train/
â”‚   â”‚
â”‚   â””â”€â”€ output/
â”‚       â”œâ”€â”€ multi_step_prompts/
â”‚       â”‚   â”œâ”€â”€ TOT_dfs/
â”‚       â”‚   â”œâ”€â”€ Open_IE_prompt/
â”‚       â”‚   â””â”€â”€ general_extraction_prompt/
â”‚       â”‚
â”‚       â”œâ”€â”€ evaluator_filtered_output/
â”‚       â””â”€â”€ metrics_evaluation/
â”‚           â”œâ”€â”€ dbpedia/
â”‚           â””â”€â”€ wikidata/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Synthetic_train_data_generation_7B.py
â”‚   â”œâ”€â”€ Llama_finetuned.py
â”‚   â”œâ”€â”€ evaluator.py
â”‚   â””â”€â”€ multi_prompt_extractor/
â”‚       â”œâ”€â”€ Open_IE_prompt.py
â”‚       â””â”€â”€ general_extraction_prompt.py
â”‚
â””â”€â”€ README.md
