# MULTI_PROMPT_TEXT2KG

This repository contains the official implementation of our research framework for **Ontology-Aware Knowledge Graph Construction from Unstructured Text using Large Language Models (LLMs)**.

The system combines multiple prompting strategies with a hierarchical evaluator to improve extraction accuracy while mitigating hallucinations. It is designed and evaluated on the **Text2KGBench** benchmark.

---

# ğŸš€ Overview

Knowledge Graph (KG) construction from natural language is challenging due to:

- Incomplete supervision  
- Ontology constraints  
- Hallucinated entities and relations  
- Inconsistent triple formatting  

To address these issues, we propose a **multi-prompt ensemble framework** consisting of:

- ğŸ” **Structured Multi-Step Reasoning (ToT-based)**
- ğŸ“š **Ontology-Constrained OpenIE Prompt**
- âš¡ **General Ontology-Aware Extraction Prompt**
- ğŸ§  **Hierarchical Evaluator (Rules Aâ€“C)**

The evaluator filters candidate triples using cross-prompt agreement, explicit evidence scoring, and textual similarity measures to reduce hallucinations and enforce schema compliance.

---

# ğŸ“‚ Data Directory

## ğŸŸ¢ Input Data

### ğŸ“‚ [`input_text`](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/input/input_text)

Contains raw input sentences for both **DBpediaâ€“WebNLG** and **Wikidataâ€“TekGen** used during inference and evaluation.

- [**DBpedia**](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/input/input_text/dbpedia)  
- [**Wikidata**](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/input/input_text/wikidata)  

---

### ğŸ“‚ [`ground_truth`](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/input/ground_truth)

Gold standard SPO triples used to compute **Precision, Recall, and F1-score**.

- [**DBpedia**](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/input/ground_truth/dbpedia)  
- [**Wikidata**](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/input/ground_truth/wikidata)  

---

### ğŸ“‚ [`fewshots_example`](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/input/fewshots_example)

Few-shot examples injected into prompts to guide ontology-aligned triple extraction.

- [**DBpedia**](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/input/fewshots_example/dbpedia)  
- [**Wikidata**](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/input/fewshots_example/wikidata)  

---

### ğŸ“‚ [`ontology`](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/input/ontology/old_ontology)

Domain-specific ontology schemas including:

- Concept definitions  
- Relation signatures  
- Domainâ€“range constraints  

These are injected directly into prompts to enforce schema compliance.

- [**DBpedia Ontologies**](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/input/ontology/old_ontology/dbpedia)  
- [**Wikidata Ontologies**](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/input/ontology/old_ontology/wikidata)  

---

### ğŸ“‚ [`train_data`](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/train_data)

Combined and enriched training dataset used for **LLaMA-3 fine-tuning**.

- [**DBpedia Training Data**](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/train_data/dbpedia)

- **Wikidata Training Data (Synthetic Enrichment Pipeline)**  

  - [Input Wikidata Train Data](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/train_data/wikidata/synthetic_train_data/wikidata_input_train)  
  - [Generated & Filtered Output Train Data Used for Fine-Tuning](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/train_data/wikidata/synthetic_train_data/wikidata_output_train)

The Wikidata dataset is synthetically enriched to compensate for incomplete distant supervision. The filtered output data is directly used during LLaMA-3 supervised fine-tuning.

---

# ğŸ“¤ Output Data

This section contains all generated outputs from the multi-prompt extraction pipeline, evaluator filtering stage, and final evaluation metrics.

## ğŸ” Multi-Prompt Extraction Outputs

Each prompting strategy generates an independent candidate triple set before evaluator filtering.

### ğŸ“‚ [`TOT_dfs`](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/output/multi_step_prompts/TOT_dfs)

Structured **Tree-of-Thoughts (ToT)-based depth-first search extraction** outputs.

---

### ğŸ“‚ [`Open_IE_prompt`](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/output/multi_step_prompts/Open_IE_prompt)

Outputs from the **Ontology-Constrained Open Information Extraction** prompt.

---

### ğŸ“‚ [`general_extraction_prompt`](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/output/multi_step_prompts/general_extraction_prompt)

Outputs from the **General Ontology-Aware Extraction Prompt**.

---

## ğŸ§  Evaluator-Filtered Outputs

### ğŸ“‚ [`evaluator_filtered_output`](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/output/evaluator_filtered_output)

Final merged and filtered triple sets after applying:

- Rule A â€“ Cross-Prompt Consensus  
- Rule B â€“ Evidence-Based Validation  
- Rule C â€“ Similarity-Based Filtering  

---

## ğŸ“Š Evaluation Results

### ğŸ“‚ [`metrics_evaluation`](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/output/metrics_evaluation)

Dataset-wise results:

- [**DBpediaâ€“WebNLG**](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/output/metrics_evaluation/dbpedia)  
- [**Wikidataâ€“TekGen**](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/data/output/metrics_evaluation/wikidata)

Reported Metrics:

- Precision (P)  
- Recall (R)  
- F1-score (F1)  
- Ontology Conformance (OC â†‘)  
- Subject Hallucination (SH â†“)  
- Relation Hallucination (RH â†“)  
- Object Hallucination (OH â†“)  

---

# ğŸ§  Source Code (`src/`)

### ğŸ“‚ [`src`](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/tree/main/src)

Contains:

- Synthetic data generation  
- Model fine-tuning  
- Multi-prompt extraction  
- Evaluator logic  
- Evaluation metrics  

---

## ğŸ”¹ Data Preparation & Training

### ğŸ“‚ [`Synthetic_train_data_generation_7B.py`](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/blob/main/src/Synthetic_train_data_generation_7B.py)

Generates ontology-filtered synthetic triples for **Wikidataâ€“TekGen**.

---

### ğŸ“‚ [`Llama_finetuned.py`](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/blob/main/src/Llama_finetuned.py)

Performs supervised fine-tuning (SFT) of **LLaMA-3-8B-Instruct** using LoRA/QLoRA.

---

## ğŸ” Multi-Prompt Extraction Modules

### ğŸ“‚ [`Open_IE_prompt.py`](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/blob/main/src/multi_prompt_extractor/Open_IE_prompt.py)

Ontology-constrained OpenIE extraction.

---

### ğŸ“‚ [`general_extraction_prompt.py`](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/blob/main/src/multi_prompt_extractor/general_extraction_prompt.py)

Lightweight ontology-aware SPO extraction.

---

### ğŸ“‚ [`evaluator.py`](https://github.com/Balramt/MULTI_PROMPT_TEXT2KG/blob/main/src/evaluator.py)

Hierarchical triple verification engine.

---

# ğŸ“ Project Directory Structure

MULTI_PROMPT_TEXT2KG/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ input/
â”‚ â”‚ â”œâ”€â”€ input_text/
â”‚ â”‚ â”œâ”€â”€ ground_truth/
â”‚ â”‚ â”œâ”€â”€ fewshots_example/
â”‚ â”‚ â””â”€â”€ ontology/
â”‚ â”‚
â”‚ â”œâ”€â”€ train_data/
â”‚ â”‚ â”œâ”€â”€ dbpedia/
â”‚ â”‚ â””â”€â”€ wikidata/
â”‚ â”‚
â”‚ â””â”€â”€ output/
â”‚ â”œâ”€â”€ multi_step_prompts/
â”‚ â”‚ â”œâ”€â”€ TOT_dfs/
â”‚ â”‚ â”œâ”€â”€ Open_IE_prompt/
â”‚ â”‚ â””â”€â”€ general_extraction_prompt/
â”‚ â”‚
â”‚ â”œâ”€â”€ evaluator_filtered_output/
â”‚ â””â”€â”€ metrics_evaluation/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ Synthetic_train_data_generation_7B.py
â”‚ â”œâ”€â”€ Llama_finetuned.py
â”‚ â”œâ”€â”€ multi_prompt_extractor/
â”‚ â”‚ â”œâ”€â”€ Open_IE_prompt.py
â”‚ â”‚ â””â”€â”€ general_extraction_prompt.py
â”‚ â””â”€â”€ evaluator.py
â”‚
â””â”€â”€ README.md


---
