MULTI_PROMPT_TEXT2KG

This repository contains the official implementation of our research framework for Ontology-Aware Knowledge Graph Construction from Unstructured Text using Large Language Models (LLMs).

The system combines multiple prompting strategies with a hierarchical evaluator to improve extraction accuracy while mitigating hallucinations. It is designed and evaluated on the Text2KGBench benchmark (DBpediaâ€“WebNLG and Wikidataâ€“TekGen).

ğŸš€ Overview

Knowledge Graph (KG) construction from natural language is challenging due to:

Incomplete supervision

Ontology constraints

Hallucinated entities and relations

Inconsistent triple formatting

To address these issues, we propose a multi-prompt ensemble framework consisting of:

ğŸ” Structured Multi-Step Reasoning (ToT-based)

ğŸ“š Ontology-Constrained OpenIE Prompt

âš¡ General Ontology-Aware Extraction Prompt

ğŸ§  Hierarchical Evaluator (Rules Aâ€“C)

The evaluator filters candidate triples using cross-prompt agreement, explicit evidence scoring, and textual similarity measures to reduce hallucinations and enforce schema compliance.

ğŸ§  System Architecture

The pipeline follows three main stages:

1ï¸âƒ£ Multi-Prompt Candidate Generation

For each input sentence and ontology:

ToT-Based Structured Extraction

Depth-first search over candidate triples

State scoring and pruning

Structured reasoning under ontology constraints

Ontology-Constrained OpenIE

Single-pass extraction

Domainâ€“range enforcement

Type assignments and evidence spans

General Ontology-Aware Extraction

Lightweight SPO extraction

High recall

Minimal structural constraints

Each prompt produces an independent candidate triple set.

2ï¸âƒ£ Evaluator (Hierarchical Verification)

All triples are merged and filtered using three rules:

Rule A â€“ Cross-Prompt Consensus

Accept if generated by at least two prompts

Must match text grounding

Rule B â€“ Evidence-Based Validation

Accept single-prompt triples only if supported by explicit evidence spans

Rule C â€“ Similarity-Based Fallback

Low-trust filtering using surface-level textual alignment

This mechanism prioritizes:

Consensus > Evidence > Weak Similarity

3ï¸âƒ£ Final Triple Set

Only evaluator-approved triples are used for:

Precision

Recall

F1-score

Ontology Conformance (OC)

Subject Hallucination (SH)

Relation Hallucination (RH)

Object Hallucination (OH)

ğŸ§ª Example
ğŸ” Test Sentence
{
  "id": "ont_film_test_1",
  "sent": "Super Capers is a 98 minute English movie that was distributed by Roadside Attractions and Lionsgate. It was directed by Ray Griggs and edited by Stacy Katzman. The budget was $2,000,000."
}

ğŸ¼ Ontology

Example: Film Ontology
Located in:

data/dbpedia/ontology/

ğŸ“ˆ Final Extracted Triples
[
  {"subject": "Super Capers", "relation": "director", "object": "Ray Griggs"},
  {"subject": "Super Capers", "relation": "distributor", "object": "Lionsgate"},
  {"subject": "Super Capers", "relation": "budget", "object": "$2,000,000"}
]


All triples are:

âœ” Text-grounded

âœ” Ontology-compliant

âœ” Evaluator-verified

ğŸ“Š Datasets

We evaluate on Text2KGBench, which includes:

ğŸ—‚ï¸ DBpediaâ€“WebNLG

4,860 sentences

19 ontologies

Complete multi-triple annotations

ğŸ—‚ï¸ Wikidataâ€“TekGen

13,474 sentences

10 ontologies

Distant supervision (synthetically enriched during training)

Place datasets under:

data/dbpedia/
data/wikidata/

ğŸ“ Repository Structure
.
â”œâ”€â”€ prompts/                     # Prompt templates (ToT, OpenIE, General)
â”œâ”€â”€ finetuning/                  # LoRA fine-tuning scripts
â”œâ”€â”€ inference/                   # Multi-prompt extraction pipeline
â”œâ”€â”€ evaluator/                   # Rule Aâ€“C implementation
â”œâ”€â”€ metrics/                     # Performance & hallucination evaluation
â”œâ”€â”€ results/                     # Model outputs and evaluation summaries
â”œâ”€â”€ data/                        # Ontologies, ground truth, prompts
â””â”€â”€ README.md

ğŸ¤– Model Setup

We use:

LLaMA-3-8B-Instruct

4-bit QLoRA fine-tuning

HuggingFace Transformers

PEFT

BitsAndBytes

PyTorch

âš™ï¸ Installation

Clone the repository:

git clone https://github.com/your-username/MULTI_PROMPT_TEXT2KG.git
cd MULTI_PROMPT_TEXT2KG


Create environment:

conda create -n text2kg python=3.10
conda activate text2kg
pip install -r requirements.txt

ğŸš€ Running the Pipeline
1ï¸âƒ£ Fine-Tune Model (Optional)
python finetuning/train_llama3.py \
  --train_data data/ \
  --output_dir models/llama3_finetuned

2ï¸âƒ£ Run Multi-Prompt Extraction
python inference/run_pipeline.py \
  --dataset dbpedia \
  --model_path models/llama3_finetuned


Outputs:

Raw prompt generations

Merged triple sets

Evaluator-filtered final triples

3ï¸âƒ£ Evaluate Performance
python metrics/evaluate_performance.py


Computes:

Precision

Recall

F1-score

4ï¸âƒ£ Evaluate Hallucinations
python metrics/evaluate_hallucination.py


Computes:

Ontology Conformance (OC â†‘)

Subject Hallucination (SH â†“)

Relation Hallucination (RH â†“)

Object Hallucination (OH â†“)

ğŸ“ˆ Reproducing Paper Results

Expected average results:

DBpediaâ€“WebNLG

F1 â‰ˆ 0.72

OC â‰ˆ 0.98

Wikidataâ€“TekGen

F1 â‰ˆ 0.52

OH â‰ˆ 0.05

(All results computed after evaluator filtering.)

ğŸ“š Baselines

Compared against:

Alpaca-LoRA (13B)

Vicuna (13B)

T5-Large (738M)

Baseline outputs and evaluation statistics are stored under:

data/*/baselines/

ğŸ“Š Statistical Testing

We conduct Wilcoxon Signed-Rank Tests to assess statistical significance across models and datasets.

Scripts located in:

analysis/wilcoxon_tests/

ğŸ“Œ Key Contributions

Multi-prompt ensemble extraction

Tree-of-Thoughts adaptation for KG construction

Ontology-constrained prompting

Hierarchical evaluator with formal rules

Hallucination-aware filtering

Synthetic enrichment for distant supervision datasets

ğŸ“ Citation

If you use this repository, please cite:

@inproceedings{yourpaper2026,
  title={Ontology-Aware Multi-Prompt Knowledge Graph Construction with Hallucination Mitigation},
  author={...},
  year={2026}
}

ğŸ“œ License

Specify your license here (e.g., MIT License).

If you would like, I can also generate:

A short GitHub â€œAboutâ€ description (under 160 characters)

A badge-style professional version

A more minimal academic README

Or a version optimized for conference artifact submission
